import torch
from .utils import seed_everything, timing_decorator, auto_amp_inference
from .utils import get_parameter_number, set_parameter_grad_false
from diffusers import HunyuanDiTPipeline, AutoPipelineForText2Image

# pretrain="weights/stabilityai"
pretrain="weights/hunyuanDiT"
class Text2Image():
    def __init__(self, pretrain=pretrain, device="cuda:0", save_memory=False):
        '''
            save_memory: if GPU memory is low, can set it
        '''
        self.save_memory = save_memory
        self.device = device
        self.pipe = AutoPipelineForText2Image.from_pretrained(
            pretrain, 
            torch_dtype = torch.float16, 
            enable_pag = True, 
            pag_applied_layers = ["blocks.(16|17|18|19)"]
        )
        # pnj_add
        # self.pipe.enable_xformers_memory_efficient_attention()
        set_parameter_grad_false(self.pipe.transformer)
        print('text2image transformer model', get_parameter_number(self.pipe.transformer))
        if not save_memory: 
            self.pipe = self.pipe.to(device)
        self.neg_txt = "æ–‡æœ¬,ç‰¹å†™,è£å‰ª,å‡ºæ¡†,æœ€å·®è´¨é‡,ä½è´¨é‡,JPEGä¼ªå½±,PGLY,é‡å¤,ç—…æ€,æ®‹ç¼º,å¤šä½™çš„æ‰‹æŒ‡,å˜å¼‚çš„æ‰‹," \
                       "ç”»å¾—ä¸å¥½çš„æ‰‹,ç”»å¾—ä¸å¥½çš„è„¸,å˜å¼‚,ç•¸å½¢,æ¨¡ç³Š,è„±æ°´,ç³Ÿç³•çš„è§£å‰–å­¦,ç³Ÿç³•çš„æ¯”ä¾‹,å¤šä½™çš„è‚¢ä½“,å…‹éš†çš„è„¸," \
                       "æ¯å®¹,æ¶å¿ƒçš„æ¯”ä¾‹,ç•¸å½¢çš„è‚¢ä½“,ç¼ºå¤±çš„æ‰‹è‡‚,ç¼ºå¤±çš„è…¿,é¢å¤–çš„æ‰‹è‡‚,é¢å¤–çš„è…¿,èåˆçš„æ‰‹æŒ‡,æ‰‹æŒ‡å¤ªå¤š,é•¿è„–å­"
        
        # self.pipe = AutoPipelineForText2Image.from_pretrained(pretrain, torch_dtype=torch.float16)
        # self.pipe.to("cuda")

    @torch.no_grad()
    @timing_decorator('text to image')
    @auto_amp_inference
    def __call__(self, *args, **kwargs):
        if self.save_memory:
            self.pipe = self.pipe.to(self.device)
            torch.cuda.empty_cache()
            torch.set_grad_enabled(False)
            res = self.call(*args, **kwargs)
            # self.pipe = self.pipe.to("cpu")
        else:
            res = self.call(*args, **kwargs)
        torch.cuda.empty_cache()
        return res

    def call(self, prompt, seed=42, steps=12):
        '''
            inputs:
                prompr: str
                seed: int
                steps: int
            return:
                rgb: PIL.Image
        '''
        # prompt = prompt + ",ç™½è‰²èƒŒæ™¯,3Dé£æ ¼,æœ€ä½³è´¨é‡" #PNJ_remove
        # prompt = prompt + "å®é™…çš„,ç™½è‰²èƒŒæ™¯" #PNJ_remove
        print(f"This is final prompt=====: {prompt}")
        seed_everything(seed)
        generator = torch.Generator(device=self.device)
        if seed is not None: generator = generator.manual_seed(int(seed))
        rgb = self.pipe(prompt=prompt, negative_prompt=self.neg_txt, num_inference_steps=steps, 
            pag_scale=1, width=384, height=384, generator=generator, return_dict=False)[0][0]
        
        print(f"ğŸš€ Text to image called with prompt: {prompt}")
        
        # rgb = self.pipe(prompt=prompt, num_inference_steps=8, guidance_scale=8).images[0]
        
        torch.cuda.empty_cache()
        self.pipe.safety_checker = None
        return rgb
    